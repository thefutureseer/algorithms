k-NN (k-NEAREST NEIGHBOR)

The k-NN algorithm is a type of machine 
learning algorithm that can be used for 
a variety of problems, including classification 
and regression.

In classification problems, the goal is to 
predict the class or category of a new data 
point based on the features of the data point 
and the classes of labeled data points in a 
training set. For example, the k-NN algorithm 
can be used to classify emails as spam or not 
spam based on the words in the email.

MindsDB shows an easy to set up example of this:
In regression problems, the goal is to predict 
a continuous value for a new data point based 
on the features of the data point and the values 
of labeled data points in a training set. For 
example, the k-NN algorithm can be used to 
predict the price of a house based on its 
features (such as the number of bedrooms and 
bathrooms).

So, the k-NN algorithm can be used for a wide 
range of problems in which we want to make 
predictions based on similar examples in a 
labeled dataset.

1. Image Classification:
Google: 
Google's image search functionality utilizes 
k-NN algorithms to match and classify images 
based on visual similarity.
Pinterest: 
Pinterest employs k-NN algorithms to classify 
and recommend visually similar images to its users.

2. Recommender Systems:
Netflix: 
Netflix uses collaborative filtering techniques, 
including variants of k-NN, to provide personalized 
movie and TV show recommendations to its users.
Amazon:
Amazon employs k-NN algorithms to power its 
recommendation engine, suggesting products 
based on the browsing and purchasing history 
of similar customers.

3. Anomaly Detection:
PayPal: 
PayPal uses k-NN algorithms for fraud detection, 
identifying suspicious transactions by comparing 
patterns and anomalies to previously observed 
fraudulent activities.
Credit card companies: 
Major credit card companies employ k-NN algorithms 
to detect fraudulent transactions by analyzing 
spending patterns and identifying outliers.

4. Text Classification:
Google: 
Google employs k-NN algorithms for text classification 
tasks, such as spam filtering in Gmail, sentiment 
analysis in Google News, or categorization in Google Search.
Facebook: 
Facebook utilizes k-NN algorithms for content recommendation, 
including personalized news feed recommendations based on text 
similarity and user interactions.

5. Collaborative Filtering:
LinkedIn: 
LinkedIn leverages k-NN algorithms to provide 
personalized job recommendations and suggest 
relevant connections based on the profiles and 
activities of similar users.
Spotify: 
Spotify uses k-NN algorithms in its music 
recommendation system, suggesting songs and 
playlists based on the listening preferences 
of users with similar taste.

Clustering:
Google: 
Google uses k-NN algorithms for clustering 
web pages in search results. By grouping 
similar pages together based on content 
similarity, they can provide more relevant 
search results and improve user experience.
Facebook: 
Facebook utilizes k-NN algorithms for clustering 
similar user profiles and interests to enable 
targeted advertising and content recommendations.


Bioinformatics:
Illumina: 
Illumina, a leading genomics company, applies 
k-NN algorithms in bioinformatics for tasks 
such as gene expression analysis, DNA sequence 
alignment, and disease classification based on 
genetic data.
DNAnexus: 
DNAnexus is a bioinformatics platform that uses 
k-NN algorithms for analyzing genomic and biomedical 
data, aiding in tasks such as variant calling, 
population genetics, and clinical research.

These are just a few examples, 
but the k-NN algorithm can be 
applied to many different 
problems in various fields.


Explain it to a child:
Imagine you have a bunch of different 
toys that you can play with, and you 
want to decide which toy to play with 
next. Your friend has some favorite 
toys that they like to play with, and 
they've told you what kinds of toys 
they are. You can use this information 
to help you decide which toy to play 
with next.

One way to do this is to look at the 
toys that your friend likes, and see 
which toys are most similar to them. 
You can then choose a toy that is 
similar to your friend's favorite toys. 
This is kind of like saying "if my 
friend likes these toys, then I might 
like them too".

This is similar to how the k-NN 
algorithm works. The toys are like the 
data points, and the features of the 
toys (like the colors, shapes, or sizes) 
are like the attributes of the data 
points. The k-NN algorithm finds the 
k nearest neighbors to a new data point 
based on how similar they are to each 
other, and then predicts the class (like 
the type of toy) based on the classes of 
those neighbors.

So, for example, if your friend likes blue 
toys that are round and small, and you 
want to choose a new toy that is similar 
to their favorites, you might choose a blue, 
round, and small toy from your collection.

It can be thought of as a two-step process:

During the training phase, the algorithm 
simply stores the labeled data points in 
memory, without doing any additional processing.

During the prediction phase, the algorithm 
takes an unlabeled data point as input and 
calculates the distance between that point 
and every labeled data point in memory. It 
then selects the k nearest neighbors to the 
input point and assigns the input point to 
the class that is most common among those 
neighbors.

Here's a simple pseudocode representation 
of the k-NN algorithm:


function kNN(inputPoint, labeledData, k) {
  let distances = [];

  // Calculate the distance between the input point and every labeled data point
  for (let i = 0; i < labeledData.length; i++) {
    let labeledPoint = labeledData[i];
    let distance = calculateDistance(inputPoint, labeledPoint);
    distances.push({ point: labeledPoint, distance: distance });
  }

  // Sort the labeled data points by distance to the input point
  distances.sort((a, b) => a.distance - b.distance);

  // Select the k nearest neighbors
  let neighbors = distances.slice(0, k);

  // Assign the input point to the class that is most common among the k nearest neighbors
  let classCounts = {};
  for (let i = 0; i < neighbors.length; i++) {
    let neighbor = neighbors[i];
    let neighborClass = neighbor.point.class;
    if (neighborClass in classCounts) {
      classCounts[neighborClass]++;
    } else {
      classCounts[neighborClass] = 1;
    }
  }
  let predictedClass = Object.keys(classCounts).reduce((a, b) => classCounts[a] > classCounts[b] ? a : b);

  return predictedClass;
}

function calculateDistance(point1, point2) {
  let sum = 0;
  for (let i = 0; i < point1.length; i++) {
    sum += Math.pow(point1[i] - point2[i], 2);
  }
  return Math.sqrt(sum);
}

// Example usage:
let labeledData = [
  { features: [1.0, 1.1], class: 'A' },
  { features: [1.0, 1.0], class: 'A' },
  { features: [0.0, 0.0], class: 'B' },
  { features: [0.0, 0.1], class: 'B' }
];
let inputPoint = [0.5, 0.5];
let k = 3;
let predictedClass = kNN(inputPoint, labeledData, k);
console.log(predictedClass); // Output: "B"

This implementation takes an 
input point as an array of features, 
a dataset of labeled data points 
with features and classes, and a 
value for k. It returns the predicted 
class of the input point based on 
the k-NN algorithm.

Note that this implementation uses 
Euclidean distance to calculate 
the distance between points, but 
other distance metrics can also be 
used depending on the application.